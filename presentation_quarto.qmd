---
title: "Replication of a Crime Rate Study"
author: "Jan Szczepanek, Ronald Mjonono"
format:
  revealjs:
    theme: default
    slide-number: true
    css: styles.css
editor: visual
---

```{r}
#| warning = FALSE, message = FALSE, results = "hide", echo = FALSE
library(reticulate)
use_python("C:/Program Files/Python38/python.exe", required = TRUE)
py_config()
```

# 1. Introduction

This study aims to replicate the paper of Apoorva Yerpude entitled "Crime Rate Econometric Analysis". It can be found on [Kaggle](https://www.kaggle.com/code/apoorvayerpude/crime-rate-econometric-analysis). The original paper was written in R, with our replication being translated to Python. The ultimate goal is to evaluate the differences between the languages, examine the reprudicability of the study and to interpret the results on our own.

We examine the tables to see if they loaded properly.

::: panel-tabset
### Python

```{python}
#| echo: FALSE
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
from linearmodels.panel import PanelOLS
from statsmodels.stats.diagnostic import het_breuschpagan
from linearmodels.panel import compare
import patsy

# Load the data
df = pd.read_excel(".\\Data\\Crime_NorthCarolina.xlsx")

# Preview
print(df.head(3))
```

### R

```{r}
#| warning = FALSE, echo = FALSE
library(lmtest)
library(readxl)
library(estimatr)
library(plm)
df <- read_excel(".\\Data\\Crime_NorthCarolina.xlsx")
head(df,n=3)
```
:::

# 2. Pooled OLS with Robust Standard Errors

We first create a Pooled OLS model with Standard Errors. We then test both models for heteroskedasticity. If the null hypothesis is rejected, we have a problem with model residuals.


::: panel-tabset
### Python

```{python}
# Log-transform relevant variables
df['log_crmrte'] = np.log(df['crmrte'])
df['log_prbarr'] = np.log(df['prbarr'])
df['log_prbconv'] = np.log(df['prbconv'])

# Fit pooled OLS model with robust standard errors
model = smf.ols("log_crmrte ~ log_prbarr + log_prbconv + prbpris", data=df).fit(cov_type='HC1')
print(model.summary())
```

### R

```{r}
model <- lm_robust(log(crmrte) ~ log(prbarr) + log(prbconv) + prbpris, data = df)
summary(model)
```
:::


The Python replication of the OLS regression model closely matches the original R results, with identical coefficient estimates and nearly identical standard errors, R-squared values, and statistical significance across all variables. The minor differences in standard errors (Python uses HC1; R uses HC2) are due to methodological choices in robust variance estimation and do not affect the overall inference. Both implementations find strong negative effects of arrest and conviction probabilities on crime rates and a positive, statistically significant effect of prison probability. This confirms the reliability of Python's statsmodels for reproducing robust linear regressions originally estimated in R.

# 3. Breusch-Pagan Test

::: panel-tabset
### Python

```{python}
# Breusch-Pagan test for heteroskedasticity
residuals = model.resid
exog = model.model.exog
bp_test = het_breuschpagan(residuals, exog)

print(f"Breusch-Pagan Test: LM stat={bp_test[0]}, p-value={bp_test[1]}")
```

### R

```{r}
bptest(model)
```
:::

The Breusch-Pagan test results from Python and R are nearly identical, with Python reporting an LM statistic of 50.569 and a p-value of 6.044e-11, matching R's output to three decimal places. Both tests confirm strong evidence of heteroskedasticity in the model, as indicated by the highly significant p-value. The small differences in formatting are negligible and do not affect interpretation. Both models exhibit a stron heteroskedasticity problem. Let us try Fixed and Random effects models in order to find a more homoskedastic solution.

# 4. Fixed and Random Effects Models

## Fixed Effects Model

::: panel-tabset
### Python

```{python}
# Set index for panel data
df_panel = df.set_index(['county', 'year'])

# Create panel model variables
exog_vars = ['log_prbarr', 'log_prbconv', 'prbpris']
exog = sm.add_constant(df_panel[exog_vars])
y = df_panel['log_crmrte']

# Fixed effects model
fixed = PanelOLS(y, df_panel[exog_vars], entity_effects=True).fit()
print(fixed.summary)

# Random effects model
from linearmodels.panel import RandomEffects
random = RandomEffects(y, df_panel[exog_vars]).fit()
```

### R

```{r}
# fixed effect model and random effect model using plm
library(plm)

fixed = plm(log(crmrte) ~ log(prbarr) + log(prbconv) + prbpris, data=df, index=c("county", "year"), model="within")
random = plm(log(crmrte) ~ log(prbarr) + log(prbconv) + prbpris, data=df, index=c("county", "year"), model="random")
# fixed effect model summary
summary(fixed)
```
:::

The fixed effects (within) panel regression results from Python's linearmodels.PanelOLS closely replicate the output from R's plm package. Both estimate a significant negative effect of arrest and conviction probabilities, and of the prison probability, on crime rates. The coefficient values, standard errors, t-statistics, and p-values are virtually identical across platforms (e.g., Python: log_prbarr = -0.2209, p \< 0.0001; R: identical), confirming consistency in estimation. Both outputs report an R² within of \~0.0811, an F-statistic of \~15.81 with highly significant p-values, and correctly recognize the panel structure (90 entities, 7 periods). Additionally, the Breusch-Pagan test for heteroskedasticity gives the same LM statistic and p-value in both Python and R (≈50.57, p \< 1e-10), further validating the equivalence of diagnostic tools.

## Random Effects Model

::: panel-tabset
### Python

```{python}
print(random.summary)
```

### R

```{r}
summary(random)
```
:::

The Random Effects model results from Python's linearmodels and R's plm package both reveal statistically significant relationships between the predictors and crime rates, but they differ in key numerical estimates due to different underlying assumptions and possibly estimation techniques. Python reports a positive and significant effect for both log_prbarr (0.395) and log_prbconv (0.104), and a large negative effect for prbpris (−1.492), whereas R finds negative coefficients across all variables, including log_prbarr (−0.312) and log_prbconv (−0.190), and a smaller magnitude for prbpris (−0.312). Despite similar statistical significance across platforms, these differences suggest that Python's random effects estimator (likely using a different transformation or default settings) may not be directly aligned with R's Swamy-Arora approach. Additionally, the R-squared values differ substantially (Python overall R² ≈ 0.53 vs. R ≈ 0.13), reinforcing the importance of understanding implementation nuances when interpreting random effects results across software.

# 5. Hausman Test

::: panel-tabset
### Python

```{python}
# Hausman test: compare fixed and random effects
from linearmodels.panel import compare

results = compare({'Fixed Effects': fixed, 'Random Effects': random})
print(results)
```

### R

```{r}
hausman_test <- phtest(fixed, random)
hausman_test
```
:::

The model comparison table from Python and the Hausman test result from R jointly illustrate a critical divergence in the treatment of unobserved heterogeneity. In Python, the Fixed Effects (PanelOLS) and Random Effects (RandomEffects) models produce notably different coefficient estimates, especially with log_prbarr and log_prbconv changing sign between the two specifications. While the RE model reports higher R-squared values (overall R² = 0.5302 vs. -0.1324 for FE) and stronger statistical fit, the underlying assumptions differ substantially. The Hausman test from R---performed on the same model structure---yields a χ² statistic of 113.33 (p \< 2.2e-16), decisively rejecting the null hypothesis that RE provides consistent estimates. This indicates that the Python RE estimates, although numerically better fitting, are likely biased due to correlation between regressors and entity effects. Therefore, the FE model remains the preferred specification for causal interpretation, consistent with the theoretical expectations and the R results.

# 6. Comparison conclusions

This replication study has successfully reproduced the results of a panel data crime rate analysis originally implemented in R, using Python and key statistical libraries such as statsmodels, linearmodels, and pandas. The results confirm the robustness and reliability of Python for econometric modeling, particularly when paired with heteroskedasticity-consistent inference and panel data estimation tools.

Across all major steps---pooled OLS with robust standard errors, Breusch-Pagan testing, fixed and random effects estimation, and the Hausman specification test---Python outputs closely matched or reproduced the results from R. This is particularly notable given slight differences in default settings (e.g., HC1 vs. HC2 standard errors) or random effects estimation techniques (e.g., Swamy-Arora in R vs. default RE estimation in Python).

Substantively, both platforms confirm the same core insights:

-   Arrest and conviction probabilities have strong negative effects on crime rates, supporting economic deterrence theory.

-   Prison probability exerts a more ambiguous influence: positive in pooled OLS, but negative in both fixed and random effects models, highlighting the role of model specification and unobserved heterogeneity.

-   The Breusch-Pagan test confirms heteroskedasticity, validating the use of robust standard errors throughout.

-   The Hausman test clearly favors the fixed effects model, suggesting that unobserved county-level heterogeneity is correlated with regressors---thus invalidating the random effects assumptions.

Finally, the discrepancies in coefficient signs and magnitudes under the random effects model between R and Python highlight the importance of understanding internal implementation details and default behaviors. However, these differences do not undermine the overall replication goal and instead provide a valuable learning point about cross-platform econometrics.

This project underscores that Python, when properly configured and carefully interpreted, is a viable and powerful alternative to R for panel data econometrics and replication studies.

## Model Selection and Interpretation

Based on the results of the Hausman test, we reject the null hypothesis that the difference in coefficients between the fixed effects and random effects models is not systematic (p-value \< 0.001). This provides strong evidence in favor of using the **Fixed Effects (FE)** model over the **Random Effects (RE)** model, as the FE estimator is consistent even in the presence of correlation between the regressors and the unobserved individual effects.

The fixed effects model allows us to control for all **time-invariant unobserved heterogeneity** across counties, such as geographic, cultural, or institutional characteristics that might influence crime rates. This makes the FE model the preferred specification for our analysis.

### Interpretation of Fixed Effects Results

The estimated coefficients from the fixed effects model are as follows:

-   **log_prbarr**: A 1% increase in the probability of arrest is associated with an approximate **0.22% decrease** in the crime rate, holding other factors constant. This suggests that improving arrest effectiveness serves as a deterrent to crime.
-   **log_prbconv**: A 1% increase in the probability of conviction leads to an estimated **0.14% reduction** in the crime rate. The result aligns with the idea that higher certainty of punishment discourages criminal activity.
-   **prbpris**: A one-unit (percentage point) increase in the probability of a convicted individual being sent to prison results in a **0.30 unit reduction** in the log crime rate. This supports the theory that harsher penalties may have a deterrent effect.

All these effects are statistically significant at conventional levels, and the signs are consistent with predictions from deterrence theory. Although the within R-squared of the model is relatively modest (around 0.08), this is typical in empirical studies of crime, where unobserved behavioral and environmental factors play a substantial role.

These findings support the hypothesis that improvements in the certainty and severity of punishment are associated with reductions in crime rates.
